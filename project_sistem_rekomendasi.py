# -*- coding: utf-8 -*-
"""Project sistem rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dp0Iv1xvleaRgwuH8eYbrmt7eIK5bbpx

## Import Library & Load Dataset
Tahap awal adalah mengimpor semua library Python yang dibutuhkan untuk manipulasi data, visualisasi, dan pemodelan. Dataset film dan kredit kemudian dimuat dari file CSV yang diekstrak dari file ZIP.
"""

import pandas as pd
import numpy as np
import ast
import zipfile
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel,cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from scipy.sparse.linalg import svds

zip_path = 'archive (4).zip'  # nama file zip kamu
extract_folder = 'tmdb_dataset'

if not os.path.exists(extract_folder):
    os.makedirs(extract_folder)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print(f"File berhasil diekstrak ke folder: {extract_folder}")

# Cek isi folder hasil ekstraksi
print("Isi folder tmdb_dataset:")
print(os.listdir('tmdb_dataset'))

# Load kedua dataset
movies_df = pd.read_csv('tmdb_dataset/tmdb_5000_movies.csv')
credits_df = pd.read_csv('tmdb_dataset/tmdb_5000_credits.csv')

print("✅ Dataset berhasil dimuat!")

"""# Data Understanding

Pada tahap ini, dataset `movies_df` dan `credits_df` dieksplorasi untuk memahami struktur, isi, dan missing value. Langkah-langkahnya meliputi:
- Menampilkan info umum dan contoh data.
- Mengganti nama kolom di `credits_df` (`movie_id` ke `id`) untuk persiapan merge.
- Mengecek jumlah missing value.
- Menggabungkan kedua dataset menjadi `movies_df` berdasarkan kolom 'id'.

# Univariate Exploratory Data Analysis (EDA)

Setelah data digabung, kita melakukan analisis eksplorasi univariat untuk memahami distribusi dari fitur-fitur penting secara individual. Ini meliputi:
- Memvisualisasikan distribusi skor vote average film.
- Mengekstrak dan memvisualisasikan distribusi genre film.
- Memvisualisasikan distribusi runtime (durasi) film.
- Menampilkan 10 film terpopuler berdasarkan skor popularitas.
- Memvisualisasikan korelasi antara popularitas dan vote average.
- Menganalisis tren jumlah film yang dirilis setiap tahun.
"""

# 4. Tampilkan info umum
print("\nInformasi Dataset Movies:")
print(movies_df.info())

print("\nInformasi Dataset Credits:")
print(credits_df.info())

# 5. Tampilkan contoh data
print("\nContoh Data Movies:")
print(movies_df.head(2))

print("\nContoh Data Credits:")
print(credits_df.head(2))

"""# Univariate Exploratory Data Analysis (EDA)



"""

# Tampilkan semua kolom dataframe
pd.set_option('display.max_columns', None)

# Rename kolom supaya bisa merge
credits_df.rename(columns={'movie_id': 'id'}, inplace=True)

print("Jumlah missing value per kolom:\n", movies_df.isnull().sum())

# Merge kedua dataset
movies_df = movies_df.merge(credits_df, on='id')

# Tampilkan beberapa baris pertama
movies_df.head()

#Visualisasi Distribusi Rating Film
plt.figure(figsize=(8,5))
sns.histplot(movies_df['vote_average'], bins=20, kde=True, color='skyblue')
plt.title("Distribusi Skor Vote Average Film", fontsize=14)
plt.xlabel("Vote Average")
plt.ylabel("Jumlah Film")
plt.grid(True, linestyle='--', alpha=0.4)
plt.show()

#Visualisasi Distribusi Genre
# Fungsi untuk parsing JSON-like text
def extract_genres(obj):
    try:
        return [d['name'] for d in ast.literal_eval(obj)]
    except:
        return []

movies_df['genres_list'] = movies_df['genres'].apply(extract_genres)

# Hitung semua genre
from collections import Counter
all_genres = sum(movies_df['genres_list'], [])
genre_count = Counter(all_genres)

# Visualisasi
plt.figure(figsize=(12,6))
sns.barplot(x=list(genre_count.keys()), y=list(genre_count.values()), palette='viridis')
plt.xticks(rotation=45)
plt.title("Distribusi Genre Film", fontsize=14)
plt.ylabel("Jumlah Film")
plt.grid(True, linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

#Distribusi Runtime Film
plt.figure(figsize=(8, 5))
sns.histplot(movies_df['runtime'].dropna(), bins=30, color='orange', kde=True)
plt.title('Distribusi Durasi Film (Runtime)', fontsize=14)
plt.xlabel('Durasi (menit)')
plt.ylabel('Jumlah Film')
plt.grid(True, linestyle='--', alpha=0.3)
plt.show()

#10 Film Terpopuler Berdasarkan Popularitas
popular_movies = movies_df[['title_x', 'popularity']].sort_values(by='popularity', ascending=False).head(10)

plt.figure(figsize=(10,6))
sns.barplot(data=popular_movies, x='popularity', y='title_x', palette='mako')
plt.title("10 Film dengan Skor Popularitas Tertinggi")
plt.xlabel("Skor Popularitas")
plt.ylabel("Judul Film")
plt.tight_layout()
plt.show()

#Korelasi: Popularitas vs Vote Average
plt.figure(figsize=(7, 5))
sns.scatterplot(data=movies_df, x='popularity', y='vote_average', alpha=0.6)
plt.title("Korelasi antara Popularitas dan Skor Vote")
plt.xlabel("Popularitas")
plt.ylabel("Vote Average")
plt.grid(True, linestyle='--', alpha=0.3)
plt.show()

#Jumlah Film Tiap Tahun

# Pastikan kolom release_date berupa datetime
movies_df['release_date'] = pd.to_datetime(movies_df['release_date'], errors='coerce')
movies_df['release_year'] = movies_df['release_date'].dt.year

# Hitung jumlah film per tahun
film_per_year = movies_df['release_year'].value_counts().sort_index()

plt.figure(figsize=(12,6))
sns.lineplot(x=film_per_year.index, y=film_per_year.values)
plt.title("Jumlah Film Dirilis per Tahun", fontsize=14)
plt.xlabel("Tahun Rilis")
plt.ylabel("Jumlah Film")
plt.grid(True, linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

"""# Data Preprocessing
Tahap ini fokus pada persiapan data untuk pemodelan:
- Menghapus kolom yang tidak relevan ('homepage').
- Mengisi missing value pada kolom tekstual ('overview', 'tagline') dengan string kosong.
- Menghapus baris dengan missing value pada 'release_date'.
- Mengisi missing value pada 'runtime' dengan nilai median.
- Mengekstrak informasi relevan (nama genre, kata kunci, pemeran, sutradara) dari kolom yang formatnya mirip JSON string.
- Membersihkan data yang diekstrak (menghilangkan spasi dan mengubah ke huruf kecil).
- Menggabungkan semua fitur yang relevan ('genres', 'keywords', 'cast', 'crew', 'overview') menjadi satu kolom baru bernama 'tags'. Kolom 'tags' ini akan digunakan sebagai input untuk membangun model content-based.
- Membuat DataFrame baru (`movie_data`) yang hanya berisi kolom 'id', 'title', dan 'tags', serta menghapus duplikat berdasarkan judul film.

"""

# 1. Drop kolom homepage karena tidak relevan
movies_df.drop(columns=['homepage'], inplace=True)

# 2. Isi missing value overview dan tagline dengan string kosong
movies_df['overview'] = movies_df['overview'].fillna('')
movies_df['tagline'] = movies_df['tagline'].fillna('')

# 3. Drop baris dengan release_date kosong (jumlahnya sangat kecil)
movies_df = movies_df.dropna(subset=['release_date'])

# 4. Isi runtime yang kosong dengan median runtime
runtime_median = movies_df['runtime'].median()
movies_df['runtime'] = movies_df['runtime'].fillna(runtime_median)

# Cek ulang apakah masih ada missing value
print("✅ Missing value setelah dibersihkan:\n")
print(movies_df.isnull().sum())

#Proses Ekstraksi dan Pembersihan Data
# Parsing data bertipe list/dictionary (JSON string)
# Fungsi bantu
def parse_names(text):
    try:
        return [i['name'] for i in ast.literal_eval(text)]
    except:
        return []

def get_top_cast(text):
    try:
        return [i['name'] for i in ast.literal_eval(text)[:3]]  # ambil 3 aktor
    except:
        return []

def get_director(text):
    try:
        for i in ast.literal_eval(text):
            if i.get('job') == 'Director':
                return i['name']
        return ''
    except:
        return ''

movies_df['genres'] = movies_df['genres'].apply(parse_names)
movies_df['keywords'] = movies_df['keywords'].apply(parse_names)
movies_df['cast'] = movies_df['cast'].apply(get_top_cast)
movies_df['crew'] = movies_df['crew'].apply(get_director)

def clean_list(x):
    if isinstance(x, list):
        return [i.replace(" ", "").lower() for i in x]
    elif isinstance(x, str):
        return x.replace(" ", "").lower()
    return ''

movies_df['genres'] = movies_df['genres'].apply(clean_list)
movies_df['keywords'] = movies_df['keywords'].apply(clean_list)
movies_df['cast'] = movies_df['cast'].apply(clean_list)
movies_df['crew'] = movies_df['crew'].apply(clean_list)

# Pastikan overview tidak kosong
movies_df['overview'] = movies_df['overview'].fillna('')

# Gabungkan fitur jadi satu string
movies_df['tags'] = movies_df.apply(
    lambda x: ' '.join(x['genres']) + ' ' +
              ' '.join(x['keywords']) + ' ' +
              ' '.join(x['cast']) + ' ' +
              x['crew'] + ' ' +
              x['overview'],
    axis=1
)

movie_data = movies_df[['id', 'title_x', 'tags']].copy()
movie_data.rename(columns={'title_x': 'title'}, inplace=True)
movie_data.head(3)

"""# Data Preparation
Data 'tags' yang telah disiapkan perlu diubah menjadi format numerik agar bisa diukur kesamaannya.
- Menggunakan `TfidfVectorizer` untuk mengubah teks pada kolom 'tags' menjadi matriks representasi TF-IDF. Ini dilakukan dengan membatasi jumlah fitur (kata) dan menghapus *stop words* bahasa Inggris.
- Menghitung matriks kesamaan (similarity matrix) antar semua film berdasarkan matriks TF-IDF menggunakan `cosine_similarity`. Matriks ini menunjukkan seberapa mirip setiap film dengan film lainnya.
"""

# Buat vectorizer dan batasi jumlah fitur (bisa disesuaikan)
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')

# Fit dan transform kolom 'tags' menjadi vektor numerik
tfidf_matrix = tfidf.fit_transform(movie_data['tags'])

# Cek bentuk matriks TF-IDF
print("✅ TF-IDF Matrix Shape:", tfidf_matrix.shape)

# Menghitung cosine similarity antar semua film
similarity = cosine_similarity(tfidf_matrix)

# Cek bentuk similarity matrix
print("✅ Cosine Similarity Matrix Shape:", similarity.shape)

"""#  Model Development - Content-Based Filtering
Dengan matriks kesamaan yang telah dihitung, kita bisa membangun fungsi rekomendasi:
- Membuat fungsi `recommend()` yang menerima judul film sebagai input.
- Fungsi ini akan mencari indeks film tersebut dalam DataFrame `movie_data`.
- Mengambil baris kesamaan yang sesuai dari `similarity_matrix` untuk film tersebut.
- Mengurutkan film berdasarkan skor kesamaan secara menurun.
- Mengabaikan film itu sendiri (skor kesamaannya 1.0) dan mengambil sejumlah film teratas (*top_n*).

"""

def recommend(title, top_n=5):
    title = title.lower()
    if title not in movie_data['title'].str.lower().values:
        return f"🎬 Film '{title}' tidak ditemukan dalam dataset."

    index = movie_data[movie_data['title'].str.lower() == title].index[0]
    distances = list(enumerate(similarity[index]))
    sorted_distances = sorted(distances, key=lambda x: x[1], reverse=True)[1:top_n+1]

    print(f"\n📽️ Rekomendasi film mirip dengan '{movie_data.iloc[index].title}':")
    for i, (idx, score) in enumerate(sorted_distances, 1):
        print(f"{i}. {movie_data.iloc[idx].title} (Similarity: {score:.2f})")

def evaluate_recommendation(title, top_n=5):
    title_lower = title.lower()
    if title_lower not in movie_data['title'].str.lower().values:
        print(f"Film '{title}' tidak ditemukan dalam dataset.")
        return

    index = movie_data[movie_data['title'].str.lower() == title_lower].index[0]
    distances = list(enumerate(similarity[index]))
    sorted_distances = sorted(distances, key=lambda x: x[1], reverse=True)[1:top_n+1]

    recommended_titles = [movie_data.iloc[idx].title for idx, score in sorted_distances]
    similarity_scores = [score for idx, score in sorted_distances]

    print(f"Rekomendasi untuk film '{title}':")
    for i, (rec_title, score) in enumerate(zip(recommended_titles, similarity_scores), 1):
        print(f"{i}. {rec_title} (Similarity: {score:.4f})")

    avg_similarity = np.mean(similarity_scores)
    print(f"\nRata-rata skor similarity dari rekomendasi: {avg_similarity:.4f}")


# Contoh pemakaian evaluasi
evaluate_recommendation("The Dark Knight", top_n=5)